{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationary multi-armed bandit\n",
    "\n",
    "In the stationary case the reward probability $p_{t, l}$ is fixed, hence $p_{t,l} = p_l$, $ \\forall t \\in \\{1, T\\}$. We will consider here the variant of the problem in which all but the best arm have the same reward probability $p=1/2$, and the reward probability of the 'best' arm is set as $p_{max} = p + \\epsilon$, where $\\epsilon \\in \\left(0, \\frac{1}{2}\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# define the generative process for rewards and outcomes with zero change probability\n",
    "from jax import devices\n",
    "devices(backend='cpu')\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import random, lax, nn, ops, vmap, jit\n",
    "\n",
    "from environment import generative_process_swtch\n",
    "rho = .0 # change probability\n",
    "log_pj_j = jnp.log(np.array([[1 - rho, rho], [1, 0]]))\n",
    "\n",
    "process = lambda *args: generative_process_swtch(*args, log_pj_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian inference\n",
    "\n",
    "Given the constrain of this investigation to the Bernoulli bandits we will define the observation likelihood as \n",
    "\n",
    "$$ p(o_t|\\vec{\\theta}, k_t) = \\prod_{k=1}^K \\left[ \\theta_k^{o_{t}}\\left( 1- \\theta_k \\right)^{1-o_{t}} \\right]^{\\delta_{k_t, k}}  $$\n",
    "\n",
    "where $a_t$ denotes the agent's choice at trial $t$. Given the Bernoulli likelihoods we will assume that both priors and posterior correspond to conjugate distribution, the Beta distribution. Hence, we can express the prior (before any observation is made) as product of prior beliefs over different arms\n",
    "\n",
    "$$ p(\\vec{\\theta}) = \\prod_{k=1}^K \\left[ \\mathcal{Be}(\\theta_k; \\alpha_{0,k}, \\beta_{0,k}) \\right]^{\\delta_{k_{t}, k}}$$\n",
    "\n",
    "where we assume that initial prior (before making any observations) corresponds to a uniform distribution, \n",
    "hence $\\alpha_{0,k}, \\beta_{0,k} = 1, \\forall \\: k$. Conjugacy of the prior allows us to define simple update rules\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\alpha_{t, k} &= \\alpha_{t-1, k} + \\delta_{k_t, k} o_t \\\\\n",
    "    \\beta_{t, k} &= \\beta_{t-1,k} + \\delta_{k_t, k} (1-o_t)\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where the parameter update is performed only for a selected $k$th arm at trial $t$. Hence, in a sequential inference setup the posterior beliefs $p(\\vec{\\theta}|o_{t:1})$ at trial $t$, become the prior beliefs at the next trial $t+1$, and can be expressed as\n",
    "\\begin{equation}\n",
    "    p(\\vec{\\theta}|o_{t:1}, k_{t+1}) = \\prod_{k=1}^K \\left[ \\mathcal{Be}\\left(\\theta_k; \\alpha_{t,k}, \\beta_{t,k}\\right) \\right]^{\\delta_{k_{t+1}, k}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the learning rule\n",
    "from learning_algos import learning_stationary as learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action selection\n",
    "\n",
    "### Thompson sampling\n",
    "\n",
    "This form of action selection algorithm is derived from the i.i.d samples from the reward probability prior\n",
    "at trial $t$, hence\n",
    "\n",
    "$$a_t = \\arg\\max_k \\theta^*_k, \\qquad \\theta^*_k \\sim \\mathcal{Be}(\\theta_k; \\alpha_{t-1, k}, \\beta_{t-1, k})$$\n",
    "\n",
    "An extension of this often found in the literature, specially on dynamic MABs, is called optimistic \n",
    "Thompson sampling, and is defined as \n",
    "\n",
    "$$a_t = \\arg\\max_k \\max(\\theta^*_k, \\mu_{t-1,k}), \\qquad \\theta^*_k \\sim \\mathcal{Be}(\\theta_k; \\alpha_{t-1, k}, \\beta_{t-1, k})$$\n",
    "\n",
    "where the expected reward probability $\\mu_{t-1, k} = \\frac{\\alpha_{t-1,k}}{\\alpha_{t-1,k} + \\beta_{t-1, k} }$\n",
    "constrains the minimal value of the sample from the prior.\n",
    "\n",
    "\n",
    "### Upper confidence bound (UCB)\n",
    "\n",
    "Another classical algorithm of reinforcement learning with a decision rule defined as\n",
    "\n",
    "\\begin{equation}\n",
    "    a_t = \\left\\{ \\begin{array}{cc}\n",
    "        \\arg\\max_k \\left(\\mu_{k, t-1} + \\sqrt{\\frac{2 \\ln t}{n_{k, t-1}}}\\right) & \\textrm{for } t>K \\\\\n",
    "        t & \\textrm{otherwise}\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu_{k, t-1} = \\frac{\\alpha_{t-1, k}}{\\nu_{t-1, k}}$, and $n_{k, t-1} = \\alpha_{t-1,k} + \\beta_{t-1,k} - \\alpha_0 - \\beta_0$. Here, we will also consider a Bayesian variant of the upper confidence bound, in which the best arm is selected as the one with the highest $z$th percentile of posterior beliefs, where the percentile increases over time as $z_t = 1 - \\frac{1}{t} $. Hence, \n",
    "\n",
    "\\begin{equation}\n",
    "    a_t = \\arg\\max_k CDF^{-1} \\left( z_t, \\alpha_{t-1}^k, \\beta_{t-1}^k \\right).\n",
    "\\end{equation}\n",
    "\n",
    "In the case of the beta distributed beliefs $\\mathcal{Be}\\left(\\alpha, \\beta\\right)$, the inverse cumulative distribution $CDF^{-1}$ corresponds to the inverse incomplete regularised beta function $I^{-1}_z \\left( \\alpha, \\beta \\right)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active inference\n",
    "\n",
    "The action selection in active inference rest upon the expected free energy $G(\\pi)$ of behavioral policy $\\pi$. Normally, behavioral policies in active inference correspond to a specific sequence of future actions $\\pi = (a_{t}, \\ldots, a_D)$ up to some planning depth $D$. Here we will limit the analysis to a shallow planning depth of $D=1$ hence each policy corresponds to one of the possible choices, that is actions $a_t$.\n",
    "The expected free energy is defined as \n",
    "\n",
    "$$ G(a_t) = D_{KL}\\left(Q(\\vec{\\theta}, k_t|a_t)||P(\\vec{\\theta}, k_t)\\right) + E_{Q(\\vec{\\theta}, k_{t}|a_t)}\\left[H[o_t|\\vec{\\theta}, k_t] \\right]$$\n",
    "\n",
    "where $P(\\vec{\\theta}, k_t)$ corresponds to a prior preference over hidden states, and $Q(\\vec{\\theta}, k_t |a_t)$ to the action dependent expectation in trial $t$, hence\n",
    "\n",
    "$$Q(\\vec{\\theta}, k_t |a_t) = p(\\vec{\\theta}|k_t, o_{1:t-1}) p(k_t|a_t) $$\n",
    "\n",
    "The expected free energy forms an upper bound on the expected surprisial $S(a_t)$ defined as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    S(a_t) & =  D_{KL}\\left(Q(o_t |a_t)||P(o_t)\\right) + E_{Q(\\vec{\\theta}, k_{t}|a_t)}\\left[H[o_t|\\vec{\\theta}, k_t] \\right] \\\\\n",
    "    & = - E_{Q(o_t|a_t)}\\left[ \\ln P(o_t) +  D_{KL}\\left( Q(\\vec{\\theta}, k_t|o_t, a_t)|| Q(\\vec{\\theta}, k_t|a_t)\\right) \\right] \\leq G(a_t)\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$$Q(o_t|a_t) = \\sum_{k_t} \\int d \\vec{\\theta} p \\left(o_t|\\vec{\\theta}, k_t\\right) Q(\\vec{\\theta}, k_t|a_t),$$ \n",
    "\n",
    "and\n",
    "\n",
    "$$ Q(\\vec{\\theta}, k_t|o_t, a_t) \\propto p(o_t|\\vec{\\theta}, k_t) Q(\\vec{\\theta}, k_t|a_t).$$\n",
    "\n",
    "We will assume that the agent has no preference between arms, hence $P(k_t) = \\frac{1}{K}$. However, the agent will prefer higher reward probabilities. We can express this as \n",
    "\n",
    "$$ P\\left(\\vec{\\theta}|k_t\\right) \\propto \\prod_k \\theta_k^{\\delta_{k_t, k} \\left( \\alpha-1 \\right)}$$\n",
    "\n",
    "From the joint preference over arms and reward probabilities we can obtain the marginal preference over outcomes as  \n",
    "\n",
    "$$ P(o_t) = \\sum_{k_t} \\int p \\left(o_t|\\vec{\\theta}, k_t\\right) P\\left(\\vec{\\theta}, k_t\\right) d \\vec{\\theta} \\propto e^{o_t \\lambda} e^{-(1-o_t) \\lambda}, \\qquad \\lambda = \\frac{1}{2} \\ln \\alpha $$\n",
    "\n",
    "We will consider two variants of the action selection rule: \n",
    "\n",
    "* one based on the expected free energy and defined as \n",
    "    $$ a_t = \\arg\\min_a G(a),$$\n",
    "\n",
    "* and another based on expected surprisal and defined as\n",
    "\n",
    "    $$ a_t = \\arg\\min_a S(a).$$\n",
    "\n",
    "The motivation for this differentiation comes from the fact that minimum of the expected free energy will not in general correspond to the minimum of the expected surprisal, hence \n",
    "\n",
    "$$ \\arg\\min_a G(a) \\neq \\arg\\min_a S(a)$$\n",
    "\n",
    "Given the known functional expressions for the prior expectation $Q\\left(\\vec{\\theta}, k_t| a_t\\right)$, and $Q\\left(o_t| a_t\\right)$ prior preferences $P\\left(\\vec{\\theta}, k_t\\right)$, and $P(o_t)$, and observation likelihood $p\\left(o_t|\\vec{\\theta}, k_t\\right)$ we get the following expressions for expected free energy and expected surprisal \n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    G(a_t = a) = & - \\ln \\left[ B(\\alpha_{t-1, a}, \\beta_{t-1, a})\\right] \\\\\n",
    "    & + (\\alpha_{t-1, a} - \\alpha - \\mu_{t-1,a}) \\psi(\\alpha_{t-1, a}) \\\\\n",
    "    & + (\\beta_{t-1, a} - 2 + \\mu_{t-1, a}) \\psi(\\beta_{t-1, a}) \\\\ & + (\\alpha + 2 - \\nu_{t-1, a}) \\psi(\\nu_{t-1, a}) - \\frac{1}{\\nu_{t-1,a}} \\\\\n",
    "    S(a_t = a) = & - \\lambda( 2 \\cdot  \\mu_{t-1, a} - 1) + \\mu_{t-1, a} \\ln \\mu_{t-1, a} + (1-\\mu_{t-1, a}) \\ln ( 1- \\mu_{t-1, a}) \\\\\n",
    "    & - \\mu_{t-1,a} \\psi(\\alpha_{t-1, a}) - (1 - \\mu_{t-1,a}) \\psi(\\beta_{t-1, a}) + \\psi(\\nu_{t-1,a}) - \\frac{1}{\\nu_{t-1,a}} \\\\\n",
    "    \\approx & -\\lambda(2 \\mu_{t-1, a} - 1) - \\frac{1}{2 \\nu_{t-1, a}} \\equiv \\tilde{S}(a_t)\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "Interestingly, the approximate form of the expected suprisal allows us to approximately express the expected epistemic affordance of each arm as\n",
    "    \\begin{equation}\n",
    "    \\begin{split}\n",
    "        E_{Q(o_t|a_t)}\\left( D_{KL}\\left[Q(\\vec{\\theta}, k_t|o_t, a_t)||Q(\\vec{\\theta}, k_t|a_t)\\right] \\right) &= E_{Q\\left(o_t, \\vec{\\theta}, k_t|a_t\\right)}\\left( \\ln \\frac{Q(\\vec{\\theta}| k_t, o_t, a_t)}{Q(\\vec{\\theta}| k_t)} + \\ln \\frac{Q(k_t| o_t, a_t)}{p(k_t|a_t)} \\right) \\\\\n",
    "        &= E_{Q\\left(o_t, \\vec{\\theta}, k_t|a_t\\right)}\\left( \\ln \\frac{Q(\\vec{\\theta}| k_t, o_t, a_t)}{Q(\\vec{\\theta}| k_t)} \\right) \\approx \\frac{1}{2 \\nu_{t-1, a_t}}\n",
    "    \\end{split}\n",
    "    \\end{equation}\n",
    "    \n",
    "Hence, the information gain of specific outcome $o_t$ can be expressed as \n",
    "\n",
    "\\begin{equation}\n",
    "    D_{KL}\\left[Q(\\vec{\\theta}, k_t|o_t, a_t)||Q(\\vec{\\theta}, k_t|a_t)\\right] = \\left\\{ \\begin{array}{ll} - \\ln \\mu_{t-1}^{a_t} + \\psi(\\alpha_{t-1}^{a_t}+1) - \\psi(\\nu_{t-1}^{a_t}+1), & \\textrm{ for } o_t=1 \\\\ - \\ln \\left( 1 - \\mu_{t-1}^{a_t}\\right) + \\psi(\\beta_{t-1}^{a_t}+1) - \\psi(\\nu_{t-1}^{a_t}+1), & \\textrm{ for } o_t = 0 \\end{array} \\right. \\approx \\left\\{ \\begin{array}{ll} \\frac{1}{2\\nu_{t-1}^{a_t}} \\left( \\frac{1}{\\mu_{t-1}^{a_t}} - 1\\right), & \\textrm{ for } o_t=1 \\\\ \\frac{1}{2\\nu_{t-1}^{a_t}} \\left( \\frac{1}{1 - \\mu_{t-1}^{a_t}} - 1\\right), & \\textrm{ for } o_t = 0 \\end{array}\\right. \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import decision algorithms\n",
    "from choice_algos import thompson_selection, ots_selection, ucb_selection, bucb_selection, efe_selection, sup_selection, app_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implements the simulator for POMDP\n",
    "def simulator(process, learning, action_selection, N=100, seed=0, steps=1000, K=10, eps=.25, save_every=100):\n",
    "    def loop_fn(t, carry):\n",
    "        rng_key, states, prior, cum_reg, cum_epst_reg = carry\n",
    "\n",
    "        rng_key, _rng_key = random.split(rng_key)\n",
    "        choices = action_selection(t, prior, _rng_key)\n",
    "\n",
    "        rng_key, _rng_key = random.split(rng_key)\n",
    "        outcomes, states = process(t, choices, states, _rng_key)\n",
    "        posterior = learning(outcomes, choices, prior)\n",
    "\n",
    "        sel = jnp.arange(N)\n",
    "\n",
    "        alphas = prior[sel, choices, 0]\n",
    "        betas = prior[sel, choices, 1]\n",
    "        nu = alphas + betas\n",
    "        mu = alphas/nu\n",
    "\n",
    "        nu_min = jnp.min(prior[..., 0] + prior[..., 1], -1)\n",
    "        cum_reg += eps * ~(choices == 0)\n",
    "\n",
    "        cum_epst_reg += (1/nu_min - 1/nu)/2\n",
    "\n",
    "        return (rng_key, states, posterior, cum_reg, cum_epst_reg)\n",
    "    \n",
    "    def sim_fn(carry, t):\n",
    "        res = lax.fori_loop(t * save_every, (t+1) * save_every, loop_fn, carry)\n",
    "        _, _, _, cum_reg, cum_epst_reg = res\n",
    "        return res, (cum_reg, cum_epst_reg)\n",
    "    \n",
    "    rng_key = random.PRNGKey(seed)\n",
    "    probs = jnp.concatenate([jnp.array([eps + .5]), jnp.ones(K-1)/2.])\n",
    "    states = [probs, jnp.zeros(1, dtype=jnp.int32)]\n",
    "    prior = jnp.ones((N, K, 2))\n",
    "    \n",
    "    _, results = lax.scan(sim_fn, (rng_key, states, prior, jnp.zeros(N), jnp.zeros(N)), jnp.arange(steps))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implements regret rate estimation over different AI algos difficulty levels\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import trange\n",
    "import itertools\n",
    "\n",
    "def estimate_regret_rate1(Ks, epsilon, steps=1000, N=1000):\n",
    "    regret_all = defaultdict(lambda: [])\n",
    "\n",
    "    seed = random.PRNGKey(100)\n",
    "    lambdas = jnp.arange(.0, 2., .05)\n",
    "    gammas = jnp.ones(1) * 1000.\n",
    "    \n",
    "    for i in trange(len(Ks), desc='K loop'):\n",
    "        K = Ks[i]\n",
    "        for func, label in zip([efe_selection, sup_selection, app_selection], \n",
    "                       ['EFE_K{}'.format(K), 'SUP_K{}'.format(K), 'APP_K{}'.format(K)]):\n",
    "\n",
    "            seed, _seed = random.split(seed)\n",
    "            sim = lambda e, g, l: simulator(process, \n",
    "                                            learning, \n",
    "                                            lambda *args: func(*args, gamma=g, lam=l), \n",
    "                                            N=N, steps=steps, K=K, \n",
    "                                            eps=e, \n",
    "                                            seed=_seed[0])\n",
    "            \n",
    "            vals = jnp.array(list(itertools.product(gammas, lambdas)))\n",
    "            \n",
    "            cum_regret, cum_epst_regret = vmap(lambda g, l: sim(epsilon, g, l))(vals[:, 0], vals[:, 1])\n",
    "            regret_all[label].append({'regret': cum_regret, 'epistemics': cum_epst_regret})\n",
    "            \n",
    "    np.savez('res_AI_Ks_e{}'.format(int(epsilon * 100)), **regret_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate1(Ks, epsilon=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate1(Ks, epsilon=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate1(Ks, epsilon=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_regret_rate2(Ks, epsilon, steps=1000, N=1000):\n",
    "    mean_reg = defaultdict(lambda: [])\n",
    "\n",
    "    seed = random.PRNGKey(100)\n",
    "\n",
    "    for i in trange(len(Ks), desc='K loop'):\n",
    "        K = Ks[i]\n",
    "        for func, label in zip([thompson_selection, ots_selection, ucb_selection], \n",
    "                       ['TS_K{}'.format(K), 'OTS_K{}'.format(K), 'UCB_K{}'.format(K)]):\n",
    "\n",
    "            seed, _seed = random.split(seed)\n",
    "\n",
    "            cum_regret, cum_epst_regret = simulator(process, \n",
    "                                                    learning, \n",
    "                                                    func, \n",
    "                                                    N=N, steps=steps, K=K, \n",
    "                                                    eps=epsilon,\n",
    "                                                    seed=_seed[0])\n",
    "                \n",
    "            mean_reg[label].append({'regret': cum_regret, 'epistemics': cum_epst_regret})\n",
    "    \n",
    "    np.savez('res_RL_Ks_e{}'.format(int(epsilon * 100)), **mean_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate2(Ks, epsilon=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate2(Ks, epsilon=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate2(Ks, epsilon=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implements the simulator for POMDP using bucb selection\n",
    "# inverse incomplete beta is not a jax function, so the code has to be in pure python\n",
    "def fori_loop(low, up, func, carry):\n",
    "    for i in range(low, up):\n",
    "        carry = func(i, carry)\n",
    "    return carry\n",
    "\n",
    "def scan(func, carry, iterate):\n",
    "    saves = []\n",
    "    for n in iterate:\n",
    "        carry, res = func(carry, n)\n",
    "        saves.append(res)\n",
    "    \n",
    "    return np.stack(saves, 0)\n",
    "\n",
    "def simulator_bucb(process, learning, action_selection, N=1000, T=1000, K=10, seed=0, eps=.25, save_every=100):\n",
    "    def loop_fn(t, carry):\n",
    "        rng_key, states, prior, cum_reg, cum_epst_reg = carry\n",
    "\n",
    "        rng_key, _rng_key = random.split(rng_key)\n",
    "        choices = action_selection(t, prior, _rng_key)\n",
    "\n",
    "        rng_key, _rng_key = random.split(rng_key)\n",
    "        outcomes, states = process(t, choices, states, _rng_key)\n",
    "        posterior = learning(outcomes, choices, prior)\n",
    "\n",
    "        sel = jnp.arange(N)\n",
    "\n",
    "        alphas = prior[sel, choices, 0]\n",
    "        betas = prior[sel, choices, 1]\n",
    "        nu = alphas + betas\n",
    "        mu = alphas/nu\n",
    "\n",
    "        nu_min = jnp.min(prior[..., 0] + prior[..., 1], -1)\n",
    "        cum_reg += eps * ~(choices == 0)\n",
    "\n",
    "        cum_epst_reg += (1/nu_min - 1/nu)/2\n",
    "\n",
    "        return (rng_key, states, posterior, cum_reg, cum_epst_reg)\n",
    "    \n",
    "    def sim_fn(carry, t):\n",
    "        res = fori_loop(t * save_every, (t+1) * save_every, loop_fn, carry)\n",
    "        _, _, _, cum_reg, cum_epst_reg = res\n",
    "        return res, (cum_reg, cum_epst_reg)\n",
    "    \n",
    "    rng_key = random.PRNGKey(seed)\n",
    "    probs = np.concatenate([np.array([eps + .5]), np.ones(K-1)/2.])\n",
    "    states = [probs, np.zeros(1, dtype=np.int32)]\n",
    "    prior = np.ones((N, K, 2))\n",
    "    \n",
    "    results = scan(sim_fn, (rng_key, states, prior, np.zeros(N), np.zeros(N)), np.arange(T))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def estimate_regret_rate_bucb(Ks, epsilon, T=10000, N=500):\n",
    "    mean_reg = defaultdict(lambda: [])\n",
    "\n",
    "    seed = random.PRNGKey(100)\n",
    "    times = np.arange(1, T+1)[::10, None]\n",
    "\n",
    "    for i in trange(len(Ks), desc='K loop'):\n",
    "        K = Ks[i]\n",
    "        for func, label in zip([bucb_selection], \n",
    "                       ['BUCB_K{}'.format(K)]):\n",
    "\n",
    "            seed, _seed = random.split(seed)\n",
    "            res = simulator_bucb(process, \n",
    "                                 learning, \n",
    "                                 func, \n",
    "                                 N=N, T=T, K=K, \n",
    "                                 eps=epsilon,\n",
    "                                 seed=_seed[0])\n",
    "            \n",
    "            regret = res[:, 0]\n",
    "            info_gain = res[:, 1]\n",
    "                \n",
    "            cum_regret = np.cumsum(regret.astype(jnp.float32), -2)[::10]\n",
    "            cum_ig = np.cumsum(info_gain.astype(jnp.float32), -2)[::10]\n",
    "            mean_reg[label].append({'regret': cum_regret, 'epistemics': cum_ig})\n",
    "\n",
    "    np.savez('res_BUCB_Ks_e{}'.format(int(epsilon * 100)), **mean_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb479a8033f4f059404815ca1980bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='K loop', max=5.0, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate_bucb(Ks, epsilon=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate_bucb(Ks, epsilon=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate_bucb(Ks, epsilon=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "* establish functional or numerical relationship between $\\lambda^*$ and $K$, $\\epsilon$.\n",
    "* determine the optimality relation for AI algorithms $\\lambda^* = f(K, \\epsilon)$? \n",
    "* introduce learning o $\\lambda$. Would the learning find values of $\\lambda$ that minimize regret?"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
