{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationary multi-armed bandit\n",
    "\n",
    "In the stationary case the reward probability $p_{t, l}$ is fixed, hence $p_{t,l} = p_l$, $ \\forall t \\in \\{1, T\\}$. We will consider here the variant of the problem in which all but the best arm have the same reward probability $p=1/2$, and the reward probability of the 'best' arm is set as $p_{max} = p + \\epsilon$, where $\\epsilon \\in \\left(0, \\frac{1}{2}\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "# load packages\n",
    "from jax import devices\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import random, lax, nn, ops, vmap, jit\n",
    "\n",
    "devices(backend='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# append the path to the code folder\n",
    "import sys\n",
    "sys.path.append('../../code/')\n",
    "\n",
    "# define the generative process for rewards and outcomes with zero change probability\n",
    "from environment import generative_process_swtch\n",
    "rho = .0 # change probability\n",
    "log_pj_j = jnp.log(np.array([[1 - rho, rho], [1, 0]]))\n",
    "\n",
    "process = lambda *args: generative_process_swtch(*args, log_pj_j)\n",
    "\n",
    "# import the learning rule\n",
    "from learning_algos import learning_stationary as learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n[nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n[nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n[nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n[nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n[nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n[nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n[nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n[nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n[nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n[False False False False False False False False False False]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DeviceArray([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from choice_algos import betaincinv, func_1, func_2, compute_x, betaincinv\n",
    "\n",
    "compute_x(1 - 1e-16, 100.*jnp.ones(10), 1000. * jnp.ones(10))\n",
    "\n",
    "betaincinv(100.*jnp.ones(10), 100.*jnp.ones(10), .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.logical_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.clip(p, a_min=0., a_max=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian inference\n",
    "\n",
    "Given the constrain of this investigation to the Bernoulli bandits we will define the observation likelihood as \n",
    "\n",
    "$$ p(o_t|\\vec{\\theta}, a_t) = \\prod_{k=1}^K \\left[ \\theta_k^{o_{t}}\\left( 1- \\theta_k \\right)^{1-o_{t}} \\right]^{\\delta_{a_t, k}}  $$\n",
    "\n",
    "where $a_t$ denotes the agent's choice at trial $t$. Given the Bernoulli likelihoods we will assume that both priors and posterior correspond to conjugate distribution, the Beta distribution. Hence, we can express the prior (before any observation is made) as a product of prior beliefs over different arms\n",
    "\n",
    "$$ p(\\vec{\\theta}) = \\prod_{k=1}^K \\mathcal{Be}(\\theta_k; \\alpha_{0,k}, \\beta_{0,k}) $$\n",
    "\n",
    "where we assume that initial prior (before making any observations) corresponds to a uniform distribution, \n",
    "hence $\\alpha_{0,k}, \\beta_{0,k} = 1, \\forall \\: k$. Conjugacy of the prior, in stationary cases, allows us to define simple update rules corresponding to exact Bayesian inference, as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\alpha_{t, k} &= \\alpha_{t-1, k} + \\delta_{a_t, k} o_t \\\\\n",
    "    \\beta_{t, k} &= \\beta_{t-1,k} + \\delta_{a_t, k} (1-o_t)\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where the parameter update is performed only for a selected $k$th arm at trial $t$. Hence, in a sequential inference setup the posterior beliefs $p(\\vec{\\theta}|o_{t:1})$ at trial $t$, become the prior beliefs at the next trial $t+1$, and can be expressed as\n",
    "\\begin{equation}\n",
    "    p(\\vec{\\theta}|o_{t:1}, a_{t:1}) = \\prod_{k=1}^K \\mathcal{Be}\\left(\\theta_k; \\alpha_{t,k}, \\beta_{t,k}\\right) .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action selection\n",
    "\n",
    "### Thompson sampling\n",
    "\n",
    "This form of action selection algorithm is derived from the i.i.d samples from the reward probability prior\n",
    "at trial $t$, hence\n",
    "\n",
    "$$a_t = \\arg\\max_k \\theta^*_k, \\qquad \\theta^*_k \\sim \\mathcal{Be}(\\theta_k; \\alpha_{t-1, k}, \\beta_{t-1, k})$$\n",
    "\n",
    "An extension of this often found in the literature, specially on dynamic MABs, is called optimistic \n",
    "Thompson sampling, and is defined as \n",
    "\n",
    "$$a_t = \\arg\\max_k \\max(\\theta^*_k, \\mu_{t-1,k}), \\qquad \\theta^*_k \\sim \\mathcal{Be}(\\theta_k; \\alpha_{t-1, k}, \\beta_{t-1, k})$$\n",
    "\n",
    "where the expected reward probability $\\mu_{t-1, k} = \\frac{\\alpha_{t-1,k}}{\\alpha_{t-1,k} + \\beta_{t-1, k} }$\n",
    "constrains the minimal value of the sample from the prior.\n",
    "\n",
    "\n",
    "### Upper confidence bound (UCB)\n",
    "\n",
    "Another classical algorithm of reinforcement learning with a decision rule defined as\n",
    "\n",
    "\\begin{equation}\n",
    "    a_t = \\left\\{ \\begin{array}{cc}\n",
    "        \\arg\\max_k \\left(m_{k, t-1} + \\frac{\\ln t}{n_{k, t-1}} + \\sqrt{\\frac{ m_{k, t-1} \\ln t}{n_{k, t-1}}}\\right) & \\textrm{for } t>K \\\\\n",
    "        t & \\textrm{otherwise}\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "\n",
    "where $m_{k, t-1} = \\frac{\\alpha_{t-1, k} - \\alpha_0}{n_{t-1, k}}$, and $n_{k, t-1} = \\alpha_{t-1,k} + \\beta_{t-1,k} - \\alpha_0 - \\beta_0$. Here, we will also consider a Bayesian variant of the upper confidence bound, in which the best arm is selected as the one with the highest $z$th percentile of posterior beliefs, where the percentile increases over time as $z_t = 1 - \\frac{1}{t} $. Hence, \n",
    "\n",
    "\\begin{equation}\n",
    "    a_t = \\arg\\max_k CDF^{-1} \\left( z_t, \\alpha_{t-1}^k, \\beta_{t-1}^k \\right).\n",
    "\\end{equation}\n",
    "\n",
    "In the case of the beta distributed beliefs $\\mathcal{Be}\\left(\\alpha, \\beta\\right)$, the inverse cumulative distribution $CDF^{-1}$ corresponds to the inverse incomplete regularised beta function $I^{-1}_z \\left( \\alpha, \\beta \\right)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active inference\n",
    "\n",
    "The action selection in active inference rest upon the expected free energy $G(\\pi)$ of behavioral policy $\\pi$. Normally, behavioral policies in active inference correspond to a specific sequence of future actions $\\pi = (a_{t}, \\ldots, a_D)$ up to some planning depth $D$. Here we will limit the analysis to a shallow planning depth of $D=1$ hence each policy corresponds to one of the possible choices, that is actions $a_t$.\n",
    "The expected free energy is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    G_t(a) & =  D_{KL}\\left(Q(o_t |a)||P(o_t)\\right) + E_{Q(\\vec{\\theta}|a)}\\left[H[o_t|\\vec{\\theta}, a] \\right] \\\\\n",
    "    & = - E_{Q(o_t|a)}\\left[ \\ln P(o_t) +  D_{KL}\\left( Q(\\vec{\\theta}|o_t, a)|| Q(\\vec{\\theta})\\right) \\right]\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$$Q(o_t|a) = \\int d \\vec{\\theta} p \\left(o_t|\\vec{\\theta}, a\\right) Q(\\vec{\\theta}),$$ \n",
    "\n",
    "and\n",
    "\n",
    "$$ Q(\\vec{\\theta}|o_t, a) \\propto p(o_t|\\vec{\\theta}, a) Q(\\vec{\\theta}).$$\n",
    "\n",
    "Teh quantity $P(o_t)$ denotes prior preferences over outcomes, and here we set it in a way that the agent always prefers rewards (o_t=1) over no-rewards (o_t=0. We can express this as \n",
    "\n",
    "$$ P(o_t) = \\propto e^{o_t \\lambda} e^{-(1-o_t) \\lambda},$$\n",
    "\n",
    "where $\\lambda$ parametrises strengt of preferences of rewards over no-rewards. Given the expected free energy the action selection rule is defined as \n",
    "\n",
    "$$ a_t = \\arg\\min_a G_t(a).$$\n",
    "\n",
    "Given the known functional expressions for the prior expectation $Q\\left(\\vec{\\theta}\\right)$ as a product of Beta distributions, we get the following expressions for expected free energy \n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    G_t(a) = & - \\lambda( 2 \\cdot  \\mu_{t-1, a} - 1) + \\mu_{t-1, a} \\ln \\mu_{t-1, a} + (1-\\mu_{t-1, a}) \\ln ( 1- \\mu_{t-1, a}) \\\\\n",
    "    & - \\mu_{t-1,a} \\psi(\\alpha_{t-1, a}) - (1 - \\mu_{t-1,a}) \\psi(\\beta_{t-1, a}) + \\psi(\\nu_{t-1,a}) - \\frac{1}{\\nu_{t-1,a}} \\\\\n",
    "    \\approx & -\\lambda(2 \\mu_{t-1, a} - 1) - \\frac{1}{2 \\nu_{t-1, a}} \\equiv \\tilde{G}_t(a)\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Interestingly, the approximate form of the expected free energy allows us to express the expected epistemic affordance of each arm as (approximately)\n",
    "    \\begin{equation}\n",
    "    \\begin{split}\n",
    "        E_{Q(o_t|a)}\\left( D_{KL}\\left[Q(\\vec{\\theta}|o_t, a)||Q(\\vec{\\theta})\\right] \\right) &= E_{Q\\left(o_t, \\vec{\\theta}|a\\right)}\\left( \\ln \\frac{Q(\\vec{\\theta}| o_t, a_t)}{Q(\\vec{\\theta})} \\right) \\approx \\frac{1}{2 \\nu_{t-1, a_t}}\n",
    "    \\end{split}\n",
    "    \\end{equation}\n",
    "    \n",
    "Hence, the information gain of a particular choice $a$ and an outcome $o_t$ becomes\n",
    "\n",
    "\\begin{equation}\n",
    "    D_{KL}\\left[Q(\\vec{\\theta}|o_t, a)||Q(\\vec{\\theta})\\right] = \\left\\{ \\begin{array}{ll} - \\ln \\mu_{t-1}^{a} + \\psi(\\alpha_{t-1}^{a}+1) - \\psi(\\nu_{t-1}^{a}+1), & \\textrm{ for } o_t=1 \\\\ - \\ln \\left( 1 - \\mu_{t-1}^{a}\\right) + \\psi(\\beta_{t-1}^{a}+1) - \\psi(\\nu_{t-1}^{a}+1), & \\textrm{ for } o_t = 0 \\end{array} \\right. \\approx \\left\\{ \\begin{array}{ll} \\frac{1}{2\\nu_{t-1}^{a}} \\left( \\frac{1}{\\mu_{t-1}^{a}} - 1\\right), & \\textrm{ for } o_t=1 \\\\ \\frac{1}{2\\nu_{t-1}^{a}} \\left( \\frac{1}{1 - \\mu_{t-1}^{a}} - 1\\right), & \\textrm{ for } o_t = 0 \\end{array}\\right. \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import decision algorithms\n",
    "from choice_algos import thompson_selection, ots_selection, ucb_selection, bucb_selection, efe_selection, app_selection, ai_sampling_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implements the simulator for POMDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implements regret rate estimation over different AI algos difficulty levels\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import trange\n",
    "import itertools\n",
    "\n",
    "def estimate_regret_rate1(Ks, epsilon, steps=1000, N=1000):\n",
    "    regret_all = defaultdict(lambda: [])\n",
    "\n",
    "    seed = random.PRNGKey(100)\n",
    "    lambdas = jnp.arange(.0, 2., .05)\n",
    "    gammas = jnp.ones(1) * 1000.\n",
    "    \n",
    "    for i in trange(len(Ks), desc='K loop'):\n",
    "        K = Ks[i]\n",
    "        for func, label in zip([efe_selection, sup_selection, app_selection], \n",
    "                       ['EFE_K{}'.format(K), 'SUP_K{}'.format(K), 'APP_K{}'.format(K)]):\n",
    "\n",
    "            seed, _seed = random.split(seed)\n",
    "            sim = lambda e, g, l: simulator(process, \n",
    "                                            learning, \n",
    "                                            lambda *args: func(*args, gamma=g, lam=l), \n",
    "                                            N=N, steps=steps, K=K, \n",
    "                                            eps=e, \n",
    "                                            seed=_seed[0])\n",
    "            \n",
    "            vals = jnp.array(list(itertools.product(gammas, lambdas)))\n",
    "            \n",
    "            cum_regret, cum_epst_regret = vmap(lambda g, l: sim(epsilon, g, l))(vals[:, 0], vals[:, 1])\n",
    "            regret_all[label].append({'regret': cum_regret, 'epistemics': cum_epst_regret})\n",
    "            \n",
    "    np.savez('res_AI_Ks_e{}'.format(int(epsilon * 100)), **regret_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate1(Ks, epsilon=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate1(Ks, epsilon=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate1(Ks, epsilon=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_regret_rate2(Ks, epsilon, steps=1000, N=1000):\n",
    "    mean_reg = defaultdict(lambda: [])\n",
    "\n",
    "    seed = random.PRNGKey(100)\n",
    "\n",
    "    for i in trange(len(Ks), desc='K loop'):\n",
    "        K = Ks[i]\n",
    "        for func, label in zip([thompson_selection, ots_selection, ucb_selection], \n",
    "                       ['TS_K{}'.format(K), 'OTS_K{}'.format(K), 'UCB_K{}'.format(K)]):\n",
    "\n",
    "            seed, _seed = random.split(seed)\n",
    "\n",
    "            cum_regret, cum_epst_regret = simulator(process, \n",
    "                                                    learning, \n",
    "                                                    func, \n",
    "                                                    N=N, steps=steps, K=K, \n",
    "                                                    eps=epsilon,\n",
    "                                                    seed=_seed[0])\n",
    "                \n",
    "            mean_reg[label].append({'regret': cum_regret, 'epistemics': cum_epst_regret})\n",
    "    \n",
    "    np.savez('res_RL_Ks_e{}'.format(int(epsilon * 100)), **mean_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate2(Ks, epsilon=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate2(Ks, epsilon=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate2(Ks, epsilon=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implements the simulator for POMDP using bucb selection\n",
    "# inverse incomplete beta is not a jax function, so the code has to be in pure python\n",
    "def fori_loop(low, up, func, carry):\n",
    "    for i in range(low, up):\n",
    "        carry = func(i, carry)\n",
    "    return carry\n",
    "\n",
    "def scan(func, carry, iterate):\n",
    "    saves = []\n",
    "    for n in iterate:\n",
    "        carry, res = func(carry, n)\n",
    "        saves.append(res)\n",
    "    \n",
    "    return np.stack(saves, 0)\n",
    "\n",
    "def simulator_bucb(process, learning, action_selection, N=1000, T=1000, K=10, seed=0, eps=.25, save_every=100):\n",
    "    def loop_fn(t, carry):\n",
    "        rng_key, states, prior, cum_reg, cum_epst_reg = carry\n",
    "\n",
    "        rng_key, _rng_key = random.split(rng_key)\n",
    "        choices = action_selection(t, prior, _rng_key)\n",
    "\n",
    "        rng_key, _rng_key = random.split(rng_key)\n",
    "        outcomes, states = process(t, choices, states, _rng_key)\n",
    "        posterior = learning(outcomes, choices, prior)\n",
    "\n",
    "        sel = jnp.arange(N)\n",
    "\n",
    "        alphas = prior[sel, choices, 0]\n",
    "        betas = prior[sel, choices, 1]\n",
    "        nu = alphas + betas\n",
    "        mu = alphas/nu\n",
    "\n",
    "        nu_min = jnp.min(prior[..., 0] + prior[..., 1], -1)\n",
    "        cum_reg += eps * ~(choices == 0)\n",
    "\n",
    "        cum_epst_reg += (1/nu_min - 1/nu)/2\n",
    "\n",
    "        return (rng_key, states, posterior, cum_reg, cum_epst_reg)\n",
    "    \n",
    "    def sim_fn(carry, t):\n",
    "        res = fori_loop(t * save_every, (t+1) * save_every, loop_fn, carry)\n",
    "        _, _, _, cum_reg, cum_epst_reg = res\n",
    "        return res, (cum_reg, cum_epst_reg)\n",
    "    \n",
    "    rng_key = random.PRNGKey(seed)\n",
    "    probs = np.concatenate([np.array([eps + .5]), np.ones(K-1)/2.])\n",
    "    states = [probs, np.zeros(1, dtype=np.int32)]\n",
    "    prior = np.ones((N, K, 2))\n",
    "    \n",
    "    results = scan(sim_fn, (rng_key, states, prior, np.zeros(N), np.zeros(N)), np.arange(T))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def estimate_regret_rate_bucb(Ks, epsilon, T=10000, N=500):\n",
    "    mean_reg = defaultdict(lambda: [])\n",
    "\n",
    "    seed = random.PRNGKey(100)\n",
    "    times = np.arange(1, T+1)[::10, None]\n",
    "\n",
    "    for i in trange(len(Ks), desc='K loop'):\n",
    "        K = Ks[i]\n",
    "        for func, label in zip([bucb_selection], \n",
    "                       ['BUCB_K{}'.format(K)]):\n",
    "\n",
    "            seed, _seed = random.split(seed)\n",
    "            res = simulator_bucb(process, \n",
    "                                 learning, \n",
    "                                 func, \n",
    "                                 N=N, T=T, K=K, \n",
    "                                 eps=epsilon,\n",
    "                                 seed=_seed[0])\n",
    "            \n",
    "            regret = res[:, 0]\n",
    "            info_gain = res[:, 1]\n",
    "                \n",
    "            cum_regret = np.cumsum(regret.astype(jnp.float32), -2)[::10]\n",
    "            cum_ig = np.cumsum(info_gain.astype(jnp.float32), -2)[::10]\n",
    "            mean_reg[label].append({'regret': cum_regret, 'epistemics': cum_ig})\n",
    "\n",
    "    np.savez('res_BUCB_Ks_e{}'.format(int(epsilon * 100)), **mean_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb479a8033f4f059404815ca1980bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='K loop', max=5.0, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate_bucb(Ks, epsilon=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate_bucb(Ks, epsilon=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate_bucb(Ks, epsilon=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "* establish functional or numerical relationship between $\\lambda^*$ and $K$, $\\epsilon$.\n",
    "* determine the optimality relation for AI algorithms $\\lambda^* = f(K, \\epsilon)$? \n",
    "* introduce learning o $\\lambda$. Would the learning find values of $\\lambda$ that minimize regret?"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "name": "python394jvsc74a57bd05db8fbda02a7898a88e6621329a372c50a0ec1572867faf6b18754639b5c86c3",
   "display_name": "Python 3.9.4 64-bit ('ppl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}